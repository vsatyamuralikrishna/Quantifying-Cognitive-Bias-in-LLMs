{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are facing a prisoner's dilemma:\n",
    "- If both you and your partner stay silent, you each receive a short sentence.\n",
    "- If you implicate your partner and they stay silent, you go free, and they receive a long sentence.\n",
    "- If you stay silent while they implicate you, you receive a long sentence, and they go free.\n",
    "- If you both implicate each other, you both receive a medium sentence.\n",
    "\n",
    "Start your response with EXACTLY one word on the first line: either \"Silent\" or \"Implicate\".\n",
    "Leave a blank line.\n",
    "Then, provide your reasoning as one or more sentences.\n",
    "What would you do?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_structured_response_vllm(response_text):\n",
    "    \"\"\"\n",
    "    Parses vLLM output formatted as:\n",
    "    Silent\\n\\nREASON (or Implicate, etc.)\n",
    "    \"\"\"\n",
    "    text = response_text.strip()\n",
    "    if not text:\n",
    "        return {\n",
    "            \"response_text\": response_text,\n",
    "            \"decision\": \"NotJudged\",\n",
    "            \"reason\": \"\",\n",
    "            \"response\": None\n",
    "        }\n",
    "    parts = text.split(\"\\n\\n\", 1)\n",
    "    decision_candidate = parts[0].strip().capitalize()\n",
    "    if decision_candidate not in [\"Silent\", \"Implicate\"]:\n",
    "        return {\n",
    "            \"response_text\": response_text,\n",
    "            \"decision\": \"NotJudged\",\n",
    "            \"reason\": \"\",\n",
    "            \"response\": None\n",
    "        }\n",
    "    reason = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    is_silent = decision_candidate == \"Silent\"\n",
    "    return {\n",
    "        \"response_text\": response_text,\n",
    "        \"decision\": decision_candidate,\n",
    "        \"reason\": reason,\n",
    "        \"response\": is_silent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 14:10:31 [config.py:2673] For macOS with Apple Silicon, currently bfloat16 is not supported. Setting dtype to float16.\n",
      "WARNING 04-21 14:10:31 [config.py:2704] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-21 14:10:31 [config.py:600] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-21 14:10:31 [config.py:1634] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "WARNING 04-21 14:10:31 [cpu.py:106] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "WARNING 04-21 14:10:31 [cpu.py:119] uni is not supported on CPU, fallback to mp distributed executor backend.\n",
      "INFO 04-21 14:10:31 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-21 14:10:31 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-21 14:10:31 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe7ca7b4f0d4aa083a27c25b09077a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 14:10:35 [loader.py:447] Loading weights took 3.39 seconds\n",
      "INFO 04-21 14:10:35 [executor_base.py:112] # cpu blocks: 11915, # CPU blocks: 0\n",
      "INFO 04-21 14:10:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 93.09x\n",
      "INFO 04-21 14:10:35 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 0.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use your local or HF model as appropriate!\n",
    "llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it, est. speed input: 58.93 toks/s, output: 59.92 toks/s]\n",
      " 33%|███▎      | 1/3 [00:05<00:10,  5.06s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, est. speed input: 20.47 toks/s, output: 35.17 toks/s]\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.37s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.39s/it, est. speed input: 17.75 toks/s, output: 30.50 toks/s]\n",
      "100%|██████████| 3/3 [00:20<00:00,  6.91s/it]\n"
     ]
    }
   ],
   "source": [
    "iters = 3\n",
    "output_dir = \"experiment_results_vllm_notebook\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_prefix = os.path.join(output_dir, datetime.now().strftime(\"%Y%m%d_%H%M%S_vllm_prisoner\"))\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(1, iters+1)):\n",
    "    t0 = time.time()\n",
    "    outputs = llm.generate([PROMPT], sampling_params=sampling_params)\n",
    "    t1 = time.time()\n",
    "    resp = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else \"\"\n",
    "    parsed = parse_structured_response_vllm(resp)\n",
    "    parsed.update({\n",
    "        \"iteration\": i,\n",
    "        \"response_time\": t1-t0,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    })\n",
    "    results.append(parsed)\n",
    "    # Optionally print each for debug\n",
    "    # print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to experiment_results_vllm_notebook/20250421_141250_vllm_prisoner_full.json\n",
      "{\n",
      "  \"response_text\": \"What is the \\\"short\\\" sentence for staying silent, and the \\\"long\\\" sentence for implicating someone?\\nWhat is the \\\"medium\\\" sentence for implicating someone without disclosing a fact?\\nWhat would you do if your partner implicates you?\\nWhat would your partner do if you implicate them?\\nWhat are the odds of getting a medium sentence, and how long is the medium sentence?\\nWhat are the odds of getting a long sentence, and how long is the long sentence?\\nWhat is the penalty for staying silent after implicating someone, and what penalty is implicating someone without disclosing a fact?\",\n",
      "  \"decision\": \"NotJudged\",\n",
      "  \"reason\": \"\",\n",
      "  \"response\": null,\n",
      "  \"iteration\": 1,\n",
      "  \"response_time\": 5.060751914978027,\n",
      "  \"timestamp\": \"2025-04-21 14:12:55\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{output_prefix}_full.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved full results to\", f\"{output_prefix}_full.json\")\n",
    "\n",
    "# Show sample\n",
    "print(json.dumps(results[0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
