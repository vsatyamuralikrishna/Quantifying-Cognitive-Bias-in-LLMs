{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_hip' from 'vllm.utils' (/groups/tylermillhouse/micromamba_envs/py311/lib/python3.11/site-packages/vllm/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/groups/tylermillhouse/micromamba_envs/py311/lib/python3.11/site-packages/vllm/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marg_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_llm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/groups/tylermillhouse/micromamba_envs/py311/lib/python3.11/site-packages/vllm/engine/arg_utils.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CacheConfig, DeviceConfig, ModelConfig,\n\u001b[32m      7\u001b[39m                          ParallelConfig, SchedulerConfig, LoRAConfig)\n\u001b[32m     10\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEngineArgs\u001b[39;00m:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Arguments for vLLM engine.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/groups/tylermillhouse/micromamba_envs/py311/lib/python3.11/site-packages/vllm/config.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformers_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cpu_memory, is_hip, get_nvcc_cuda_version\n\u001b[32m     13\u001b[39m logger = init_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     15\u001b[39m _GB = \u001b[32m1\u001b[39m << \u001b[32m30\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'is_hip' from 'vllm.utils' (/groups/tylermillhouse/micromamba_envs/py311/lib/python3.11/site-packages/vllm/utils.py)"
     ]
    }
   ],
   "source": [
    "# Save this as prisoner_dilemma_comparison.ipynb\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mHUGGINGFACE_HUB_CACHE\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m/groups/tylermillhouse/huggingface/hub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Check if CUDA is available and print GPU information\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m.cuda.is_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of GPUs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.device_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Set environment variables for Hugging Face cache\n",
    "os.environ[\"HF_HOME\"] = \"/groups/tylermillhouse/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/groups/tylermillhouse/huggingface/transformers\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/groups/tylermillhouse/huggingface/hub\"\n",
    "\n",
    "# Check if CUDA is available and print GPU information\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prisoner's dilemma prompt with the modified instruction\n",
    "def create_prompt(model_type=\"llama\"):\n",
    "    \"\"\"Create a prompt formatted for the specified model type\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are facing a life-changing dilemma. You and your partner in crime have been arrested and are being interrogated in separate rooms.\n",
    "\n",
    "            You have two options: to remain silent or to implicate your partner.\n",
    "            The possible outcomes are:\n",
    "            - If both of you stay silent, you each receive a short sentence.\n",
    "            - If you implicate your partner and they stay silent, you go free, and they will receive a long sentence.\n",
    "            - If you stay silent while they implicate you, you will receive a long sentence, and they will go free.\n",
    "            - If you both implicate each other, you both receive a medium sentence.\n",
    "\n",
    "            You must decide what to do, but you can't know or predict your partner's choice in advance.\n",
    "\n",
    "            What would you do? Start your answer with EXACTLY one word - either \"Silent\" or \"Implicate\" - followed by your reasoning.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will respond with a boolean value in a JSON format where 'silent' is true and 'implicate' is false, like this: {response: true or false}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format based on model type\n",
    "    if model_type == \"llama\":\n",
    "        # Llama 2 format\n",
    "        formatted_prompt = \"<s>[INST] \"\n",
    "        for msg in messages:\n",
    "            formatted_prompt += f\"{msg['content']} \"\n",
    "        formatted_prompt += \"[/INST] \"\n",
    "\n",
    "    elif model_type == \"mistral\":\n",
    "        # Mistral format\n",
    "        formatted_prompt = \"<s>[INST] \"\n",
    "        for msg in messages:\n",
    "            formatted_prompt += f\"{msg['content']} \"\n",
    "        formatted_prompt += \"[/INST] \"\n",
    "\n",
    "    elif model_type == \"tinyllama\":\n",
    "        # TinyLlama format\n",
    "        formatted_prompt = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                formatted_prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                formatted_prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "        formatted_prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "    elif model_type == \"gemma\":\n",
    "        # Gemma format\n",
    "        formatted_prompt = \"\"\n",
    "        system_content = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_content += f\"{msg['content']}\\n\"\n",
    "        formatted_prompt = f\"<start_of_turn>user\\n{system_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    else:\n",
    "        # Generic format\n",
    "        formatted_prompt = \"\"\n",
    "        for msg in messages:\n",
    "            formatted_prompt += f\"{msg['role']}: {msg['content']}\\n\"\n",
    "        formatted_prompt += \"assistant: \"\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"TinyLlama-1.1B-Chat\",\n",
    "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"prompt_type\": \"tinyllama\",\n",
    "        \"gated\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Mistral-7B-Instruct\",\n",
    "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"prompt_type\": \"mistral\",\n",
    "        \"gated\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-2-7b-chat\",\n",
    "        \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"prompt_type\": \"llama\",\n",
    "        \"gated\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gemma-7b-it\",\n",
    "        \"model_id\": \"google/gemma-7b-it\",\n",
    "        \"prompt_type\": \"gemma\",\n",
    "        \"gated\": True\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to run a model and get response\n",
    "def run_model(model_config, tensor_parallel_size=1):\n",
    "    \"\"\"Run a model and return the structured response\"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_id = model_config[\"model_id\"]\n",
    "    prompt_type = model_config[\"prompt_type\"]\n",
    "\n",
    "    print(f\"Loading model: {model_name} ({model_id})\")\n",
    "    print(f\"Using tensor parallelism with {tensor_parallel_size} GPUs\")\n",
    "\n",
    "    # Create the prompt\n",
    "    formatted_prompt = create_prompt(prompt_type)\n",
    "\n",
    "    try:\n",
    "        # Initialize the model with GPU settings\n",
    "        llm = LLM(\n",
    "            model=model_id,\n",
    "            download_dir=\"/groups/tylermillhouse/huggingface/models\",\n",
    "            tensor_parallel_size=tensor_parallel_size,\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=0.85\n",
    "        )\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        print(\"Generating response...\")\n",
    "        start_time = time.time()\n",
    "        outputs = llm.generate([formatted_prompt], sampling_params)\n",
    "        response_time = time.time() - start_time\n",
    "        print(f\"Response generated in {response_time:.2f} seconds\")\n",
    "\n",
    "        # Extract the raw text response\n",
    "        raw_response = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "        # Parse the response to determine if the model chose to stay silent or implicate\n",
    "        # First check if it starts with \"Silent\" or \"Implicate\"\n",
    "        if raw_response.startswith(\"Silent\"):\n",
    "            decision = \"Silent\"\n",
    "        elif raw_response.startswith(\"Implicate\"):\n",
    "            decision = \"Implicate\"\n",
    "        # Fallback to checking for \"true\" or \"false\" in the response\n",
    "        elif \"true\" in raw_response.lower():\n",
    "            decision = \"Silent\"\n",
    "        elif \"false\" in raw_response.lower():\n",
    "            decision = \"Implicate\"\n",
    "        # Default fallback\n",
    "        else:\n",
    "            decision = \"Silent\" if \"silent\" in raw_response.lower() else \"Implicate\"\n",
    "\n",
    "        # Create the structured response\n",
    "        structured_response = {\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"response_text\": raw_response,\n",
    "            \"decision\": decision,\n",
    "            \"reason\": raw_response,\n",
    "            \"response\": decision == \"Silent\",  # true if Silent, false if Implicate\n",
    "            \"iteration\": 1,\n",
    "            \"response_time\": response_time,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "        return structured_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running model {model_name}: {str(e)}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models and collect results\n",
    "results = []\n",
    "\n",
    "# Determine number of GPUs to use\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(f\"Available GPUs: {num_gpus}\")\n",
    "\n",
    "# Use all available GPUs for tensor parallelism, up to 2\n",
    "tensor_parallel_size = min(2, num_gpus)\n",
    "print(f\"Using {tensor_parallel_size} GPUs for tensor parallelism\")\n",
    "\n",
    "for model_config in models:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Processing model: {model_config['name']}\")\n",
    "\n",
    "    result = run_model(model_config, tensor_parallel_size)\n",
    "    results.append(result)\n",
    "\n",
    "    # Save individual result\n",
    "    filename = f\"prisoner_dilemma_{model_config['name'].replace('-', '_').lower()}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"Result saved to {filename}\")\n",
    "\n",
    "    print(f\"Completed model: {model_config['name']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Save all results\n",
    "with open(\"prisoner_dilemma_all_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"\\nAll results saved to prisoner_dilemma_all_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize results\n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and visualize the results\"\"\"\n",
    "    # Filter out results with errors\n",
    "    valid_results = [r for r in results if \"error\" not in r]\n",
    "\n",
    "    if not valid_results:\n",
    "        print(\"No valid results to analyze\")\n",
    "        return\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(valid_results)\n",
    "\n",
    "    # Display the decisions\n",
    "    print(\"Model Decisions:\")\n",
    "    decision_df = df[[\"model\", \"decision\", \"response_time\"]]\n",
    "    print(decision_df)\n",
    "\n",
    "    # Create individual bar charts for each model\n",
    "    for model_name in df[\"model\"].unique():\n",
    "        model_data = df[df[\"model\"] == model_name]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(\n",
    "            [\"Decision\", \"Response Time (s)\"],\n",
    "            [1 if model_data[\"decision\"].iloc[0] == \"Implicate\" else 0, model_data[\"response_time\"].iloc[0]],\n",
    "            color=['#ff9999' if model_data[\"decision\"].iloc[0] == \"Implicate\" else '#66b3ff', '#aaaaaa']\n",
    "        )\n",
    "        plt.title(f'{model_name} - Decision: {model_data[\"decision\"].iloc[0]}')\n",
    "        plt.ylabel('Value')\n",
    "        plt.ylim(0, max(10, model_data[\"response_time\"].iloc[0] * 1.2))  # Adjust y-axis\n",
    "\n",
    "        # Add text labels\n",
    "        plt.text(0, 0.5, model_data[\"decision\"].iloc[0], ha='center')\n",
    "        plt.text(1, model_data[\"response_time\"].iloc[0] / 2, f\"{model_data['response_time'].iloc[0]:.2f}s\", ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{model_name.replace('-', '_').lower()}_results.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Create a comparison bar chart for all models\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(\n",
    "        df[\"model\"],\n",
    "        df[\"response_time\"],\n",
    "        color=['#ff9999' if d == \"Implicate\" else '#66b3ff' for d in df[\"decision\"]]\n",
    "    )\n",
    "    plt.title('Model Comparison - Response Time and Decision')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Response Time (seconds)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add decision labels above bars\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        plt.text(i, row[\"response_time\"] + 0.1, row[\"decision\"], ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the full responses\n",
    "    print(\"\\nFull Responses:\")\n",
    "    for result in valid_results:\n",
    "        print(f\"\\n{result['model']} ({result['decision']}):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result['response_text'])\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Run analysis if we have results\n",
    "if results:\n",
    "    analyze_results(results)\n",
    "else:\n",
    "    print(\"No results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b78e534264eb5f105d8d77bd2f3f351df49eaf52980e497575574ca823a960f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
