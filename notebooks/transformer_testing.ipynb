{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-23 03:10:57 config.py:1657] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-23 03:10:57 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post1) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 04-23 03:10:58 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-23 03:10:58 selector.py:116] Using XFormers backend.\n",
      "INFO 04-23 03:10:58 model_runner.py:997] Starting to load model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "INFO 04-23 03:10:58 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-23 03:10:58 selector.py:116] Using XFormers backend.\n",
      "INFO 04-23 03:10:59 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 04-23 03:11:20 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.80s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.80s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-23 03:11:23 model_runner.py:1008] Loading model weights took 2.0502 GB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in model execution (input dumped to /tmp/err_execute_model_input_20250423-031123.pkl): CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py:1544\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     model_forward_start\u001b[39m.\u001b[39mrecord()\n\u001b[0;32m-> 1544\u001b[0m hidden_or_intermediate_states \u001b[39m=\u001b[39m model_executable(\n\u001b[1;32m   1545\u001b[0m     input_ids\u001b[39m=\u001b[39;49mmodel_input\u001b[39m.\u001b[39;49minput_tokens,\n\u001b[1;32m   1546\u001b[0m     positions\u001b[39m=\u001b[39;49mmodel_input\u001b[39m.\u001b[39;49minput_positions,\n\u001b[1;32m   1547\u001b[0m     kv_caches\u001b[39m=\u001b[39;49mkv_caches,\n\u001b[1;32m   1548\u001b[0m     attn_metadata\u001b[39m=\u001b[39;49mmodel_input\u001b[39m.\u001b[39;49mattn_metadata,\n\u001b[1;32m   1549\u001b[0m     intermediate_tensors\u001b[39m=\u001b[39;49mintermediate_tensors,\n\u001b[1;32m   1550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mMultiModalInputs\u001b[39m.\u001b[39;49mas_kwargs(multi_modal_kwargs,\n\u001b[1;32m   1551\u001b[0m                                  device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m   1552\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseqlen_agnostic_kwargs)\n\u001b[1;32m   1554\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservability_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservability_config\u001b[39m.\u001b[39mcollect_model_forward_time):\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:448\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m    441\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    442\u001b[0m     input_ids: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    447\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[torch\u001b[39m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 448\u001b[0m     model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, positions, kv_caches,\n\u001b[1;32m    449\u001b[0m                               attn_metadata, intermediate_tensors)\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:329\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    328\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\n\u001b[0;32m--> 329\u001b[0m     hidden_states, residual \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    330\u001b[0m         positions,\n\u001b[1;32m    331\u001b[0m         hidden_states,\n\u001b[1;32m    332\u001b[0m         kv_caches[i \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_layer],\n\u001b[1;32m    333\u001b[0m         attn_metadata,\n\u001b[1;32m    334\u001b[0m         residual,\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m get_pp_group()\u001b[39m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:251\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    249\u001b[0m     hidden_states, residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(\n\u001b[1;32m    250\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 251\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    252\u001b[0m     positions\u001b[39m=\u001b[39;49mpositions,\n\u001b[1;32m    253\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    254\u001b[0m     kv_cache\u001b[39m=\u001b[39;49mkv_cache,\n\u001b[1;32m    255\u001b[0m     attn_metadata\u001b[39m=\u001b[39;49mattn_metadata,\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    258\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:181\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    180\u001b[0m q, k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[0;32m--> 181\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(q, k, v, kv_cache, attn_metadata)\n\u001b[1;32m    182\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/attention/layer.py:98\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     query: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     attn_type: AttentionType \u001b[39m=\u001b[39m AttentionType\u001b[39m.\u001b[39mDECODER,\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimpl\u001b[39m.\u001b[39;49mforward(query,\n\u001b[1;32m     99\u001b[0m                              key,\n\u001b[1;32m    100\u001b[0m                              value,\n\u001b[1;32m    101\u001b[0m                              kv_cache,\n\u001b[1;32m    102\u001b[0m                              attn_metadata,\n\u001b[1;32m    103\u001b[0m                              \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_k_scale,\n\u001b[1;32m    104\u001b[0m                              \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_v_scale,\n\u001b[1;32m    105\u001b[0m                              attn_type\u001b[39m=\u001b[39;49mattn_type)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/attention/backends/xformers.py:595\u001b[0m, in \u001b[0;36mXFormersImpl.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, k_scale, v_scale, attn_type)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m kv_cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m prefill_meta\u001b[39m.\u001b[39mblock_tables\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[39m# normal attention.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[39m# block tables are empty if the prompt does not have a cached\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     \u001b[39m# prefix.\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_memory_efficient_xformers_forward(\n\u001b[1;32m    596\u001b[0m         query, key, value, prefill_meta, attn_type\u001b[39m=\u001b[39;49mattn_type)\n\u001b[1;32m    597\u001b[0m     \u001b[39massert\u001b[39;00m out\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m output[:num_prefill_tokens]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/attention/backends/xformers.py:739\u001b[0m, in \u001b[0;36mXFormersImpl._run_memory_efficient_xformers_forward\u001b[0;34m(self, query, key, value, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m    738\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 739\u001b[0m out \u001b[39m=\u001b[39m xops\u001b[39m.\u001b[39;49mmemory_efficient_attention_forward(\n\u001b[1;32m    740\u001b[0m     query,\n\u001b[1;32m    741\u001b[0m     key,\n\u001b[1;32m    742\u001b[0m     value,\n\u001b[1;32m    743\u001b[0m     attn_bias\u001b[39m=\u001b[39;49mattn_bias[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    744\u001b[0m     p\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m    745\u001b[0m     scale\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale)\n\u001b[1;32m    746\u001b[0m \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mview_as(original_query)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:304\u001b[0m, in \u001b[0;36mmemory_efficient_attention_forward\u001b[0;34m(query, key, value, attn_bias, p, scale, op, output_dtype)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39mCalculates the forward pass of :attr:`xformers.ops.memory_efficient_attention`.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[39mreturn\u001b[39;00m _memory_efficient_attention_forward(\n\u001b[1;32m    305\u001b[0m     Inputs(\n\u001b[1;32m    306\u001b[0m         query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m    307\u001b[0m         key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    308\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    309\u001b[0m         p\u001b[39m=\u001b[39;49mp,\n\u001b[1;32m    310\u001b[0m         attn_bias\u001b[39m=\u001b[39;49mattn_bias,\n\u001b[1;32m    311\u001b[0m         scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m    312\u001b[0m         output_dtype\u001b[39m=\u001b[39;49moutput_dtype,\n\u001b[1;32m    313\u001b[0m     ),\n\u001b[1;32m    314\u001b[0m     op\u001b[39m=\u001b[39;49mop,\n\u001b[1;32m    315\u001b[0m )\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:418\u001b[0m, in \u001b[0;36m_memory_efficient_attention_forward\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    416\u001b[0m     _ensure_op_supports_or_raise(\u001b[39mValueError\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mmemory_efficient_attention\u001b[39m\u001b[39m\"\u001b[39m, op, inp)\n\u001b[0;32m--> 418\u001b[0m out, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49mapply(inp, needs_gradient\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    419\u001b[0m \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:258\u001b[0m, in \u001b[0;36mFwOp.apply\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    254\u001b[0m         bias \u001b[39m=\u001b[39m _attn_bias_apply(\n\u001b[1;32m    255\u001b[0m             inp\u001b[39m.\u001b[39mattn_bias, partial(torch\u001b[39m.\u001b[39mselect, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mgroup)\n\u001b[1;32m    256\u001b[0m         )\n\u001b[1;32m    257\u001b[0m         outs\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 258\u001b[0m             \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mapply_bmhk(\n\u001b[1;32m    259\u001b[0m                 replace(inp, query\u001b[39m=\u001b[39;49mquery, key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49mvalue, attn_bias\u001b[39m=\u001b[39;49mbias),\n\u001b[1;32m    260\u001b[0m                 needs_gradient\u001b[39m=\u001b[39;49mneeds_gradient,\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m streams[\u001b[39m1\u001b[39m:]:\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:281\u001b[0m, in \u001b[0;36mFwOp.apply_bmhk\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    280\u001b[0m seqstart_k, seqstart_q, max_seqlen_q, max_seqlen_k \u001b[39m=\u001b[39m _get_seqlen_info(inp)\n\u001b[0;32m--> 281\u001b[0m out, lse, rng_seed, rng_offset, _, _ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mOPERATOR(\n\u001b[1;32m    282\u001b[0m     query\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mquery,\n\u001b[1;32m    283\u001b[0m     key\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mkey,\n\u001b[1;32m    284\u001b[0m     value\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    285\u001b[0m     bias\u001b[39m=\u001b[39;49m_get_tensor_bias(inp\u001b[39m.\u001b[39;49mattn_bias),\n\u001b[1;32m    286\u001b[0m     cu_seqlens_q\u001b[39m=\u001b[39;49mseqstart_q,\n\u001b[1;32m    287\u001b[0m     cu_seqlens_k\u001b[39m=\u001b[39;49mseqstart_k,\n\u001b[1;32m    288\u001b[0m     max_seqlen_q\u001b[39m=\u001b[39;49mmax_seqlen_q,\n\u001b[1;32m    289\u001b[0m     max_seqlen_k\u001b[39m=\u001b[39;49mmax_seqlen_k,\n\u001b[1;32m    290\u001b[0m     dropout_p\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mp,\n\u001b[1;32m    291\u001b[0m     compute_log_sumexp\u001b[39m=\u001b[39;49mneeds_gradient,\n\u001b[1;32m    292\u001b[0m     custom_mask_type\u001b[39m=\u001b[39;49m_custom_mask_type(inp\u001b[39m.\u001b[39;49mattn_bias),\n\u001b[1;32m    293\u001b[0m     scale\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mscale,\n\u001b[1;32m    294\u001b[0m     seqlen_k\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    295\u001b[0m         inp\u001b[39m.\u001b[39;49mattn_bias\u001b[39m.\u001b[39;49mk_seqinfo\u001b[39m.\u001b[39;49mseqlen\n\u001b[1;32m    296\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(\n\u001b[1;32m    297\u001b[0m             inp\u001b[39m.\u001b[39;49mattn_bias, BlockDiagonalCausalWithOffsetPaddedKeysMask\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    299\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    300\u001b[0m     ),\n\u001b[1;32m    301\u001b[0m     window_size\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    302\u001b[0m         inp\u001b[39m.\u001b[39;49mattn_bias\u001b[39m.\u001b[39;49m_window_size\n\u001b[1;32m    303\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(\n\u001b[1;32m    304\u001b[0m             inp\u001b[39m.\u001b[39;49mattn_bias,\n\u001b[1;32m    305\u001b[0m             (\n\u001b[1;32m    306\u001b[0m                 BlockDiagonalCausalLocalAttentionMask,\n\u001b[1;32m    307\u001b[0m                 BlockDiagonalCausalLocalAttentionFromBottomRightMask,\n\u001b[1;32m    308\u001b[0m                 LowerTriangularFromBottomRightLocalAttentionMask,\n\u001b[1;32m    309\u001b[0m             ),\n\u001b[1;32m    310\u001b[0m         )\n\u001b[1;32m    311\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m     ),\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    314\u001b[0m ctx: Optional[Context] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[39mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[39mreturn\u001b[39;00m self_\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(kwargs \u001b[39mor\u001b[39;49;00m {}))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     23\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     24\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     25\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     26\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 🐍 Step 4: Initialize vLLM with model cache\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 🧪 Step 5: Run the model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py:178\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m engine_args \u001b[39m=\u001b[39m EngineArgs(\n\u001b[1;32m    157\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m )\n\u001b[0;32m--> 178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine \u001b[39m=\u001b[39m LLMEngine\u001b[39m.\u001b[39;49mfrom_engine_args(\n\u001b[1;32m    179\u001b[0m     engine_args, usage_context\u001b[39m=\u001b[39;49mUsageContext\u001b[39m.\u001b[39;49mLLM_CLASS)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter \u001b[39m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:550\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    548\u001b[0m executor_class \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    549\u001b[0m \u001b[39m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    551\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mengine_config\u001b[39m.\u001b[39;49mto_dict(),\n\u001b[1;32m    552\u001b[0m     executor_class\u001b[39m=\u001b[39;49mexecutor_class,\n\u001b[1;32m    553\u001b[0m     log_stats\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m engine_args\u001b[39m.\u001b[39;49mdisable_log_stats,\n\u001b[1;32m    554\u001b[0m     usage_context\u001b[39m=\u001b[39;49musage_context,\n\u001b[1;32m    555\u001b[0m     stat_loggers\u001b[39m=\u001b[39;49mstat_loggers,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[39mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:331\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_executor \u001b[39m=\u001b[39m executor_class(\n\u001b[1;32m    318\u001b[0m     model_config\u001b[39m=\u001b[39mmodel_config,\n\u001b[1;32m    319\u001b[0m     cache_config\u001b[39m=\u001b[39mcache_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m     observability_config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservability_config,\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39membedding_mode:\n\u001b[0;32m--> 331\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_kv_caches()\n\u001b[1;32m    333\u001b[0m \u001b[39m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:460\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_initialize_kv_caches\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \n\u001b[1;32m    456\u001b[0m \u001b[39m    The workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    and the swap CPU cache.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     num_gpu_blocks, num_cpu_blocks \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 460\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_executor\u001b[39m.\u001b[39;49mdetermine_num_available_blocks())\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_config\u001b[39m.\u001b[39mnum_gpu_blocks_override \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m         num_gpu_blocks_override \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_config\u001b[39m.\u001b[39mnum_gpu_blocks_override\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:114\u001b[0m, in \u001b[0;36mGPUExecutor.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdetermine_num_available_blocks\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Determine the number of available KV blocks by invoking the\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    underlying worker.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver_worker\u001b[39m.\u001b[39;49mdetermine_num_available_blocks()\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/worker/worker.py:223\u001b[0m, in \u001b[0;36mWorker.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    221\u001b[0m \u001b[39m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# of the model.\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_runner\u001b[39m.\u001b[39;49mprofile_run()\n\u001b[1;32m    225\u001b[0m \u001b[39m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py:1216\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m get_pp_group()\u001b[39m.\u001b[39mis_first_rank:\n\u001b[1;32m   1212\u001b[0m     intermediate_tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmake_empty_intermediate_tensors(\n\u001b[1;32m   1213\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m         dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m   1215\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_model(model_input, kv_caches, intermediate_tensors)\n\u001b[1;32m   1217\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m   1218\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/groups/tylermillhouse/micromamba_envs/vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:126\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             dumped_inputs[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marg_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m arg\n\u001b[1;32m    125\u001b[0m     pickle\u001b[39m.\u001b[39mdump(dumped_inputs, filep)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(err)(\n\u001b[1;32m    127\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError in model execution (input dumped to \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(err)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in model execution (input dumped to /tmp/err_execute_model_input_20250423-031123.pkl): CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os, uuid, time\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "# If needed, explicitly set environment variables (only for inline use; better to export in shell)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# 🧠 Step 2: Define structured prompt\n",
    "prompt = (\n",
    "    \"You are facing a life-changing dilemma. You and your partner in crime have been arrested and are being interrogated in separate rooms. \"\n",
    "    \"You have two options: to remain silent or to implicate your partner. \"\n",
    "    \"The possible outcomes are:\\n\"\n",
    "    \"- If both of you stay silent, you each receive a short sentence.\\n\"\n",
    "    \"- If you implicate your partner and they stay silent, you go free, and they will receive a long sentence.\\n\"\n",
    "    \"- If you stay silent while they implicate you, you will receive a long sentence, and they will go free.\\n\"\n",
    "    \"- If you both implicate each other, you both receive a medium sentence.\\n\"\n",
    "    \"You must decide what to do, but you can't know or predict your partner's choice in advance. \"\n",
    "    \"Start your answer with EXACTLY one word — either 'Silent' or 'Implicate' — followed by your reasoning.\"\n",
    ")\n",
    "\n",
    "# 🧰 Step 3: Sampling setup\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# 🐍 Step 4: Initialize vLLM with model cache\n",
    "llm = LLM(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    tokenizer=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    gpu_memory_utilization=0.9,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# 🧪 Step 5: Run the model\n",
    "start_time = time.time()\n",
    "outputs = llm.generate(prompts=[prompt], sampling_params=sampling_params)\n",
    "end_time = time.time()\n",
    "\n",
    "# 📦 Step 6: Extract result\n",
    "response_text = outputs[0].outputs[0].text.strip()\n",
    "print(f\"📤 Response:\\n{response_text}\\n\")\n",
    "\n",
    "# 📊 Step 7: Structure result\n",
    "result = {\n",
    "    \"execution_id\": str(uuid.uuid4()),\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt\": prompt,\n",
    "    \"response_text\": response_text,\n",
    "    \"duration_sec\": round(end_time - start_time, 2),\n",
    "    \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"engine\": \"vllm\",\n",
    "    \"device\": \"cuda:1\"\n",
    "}\n",
    "\n",
    "# 🧾 Step 8: Output result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7af8a51545e8737769067ab50f51c45dad9a97dfe7f3e741e91ceeb19bc17a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
