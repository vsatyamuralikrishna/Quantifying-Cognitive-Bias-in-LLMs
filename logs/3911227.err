Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
2025/05/02 15:07:07 routes.go:1231: INFO server config env="map[CUDA_VISIBLE_DEVICES:0,1 GPU_DEVICE_ORDINAL:0,1 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/groups/tylermillhouse/ollama/home/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0,1 http_proxy: https_proxy: no_proxy:]"
time=2025-05-02T15:07:07.931-07:00 level=INFO source=images.go:458 msg="total blobs: 36"
time=2025-05-02T15:07:07.936-07:00 level=INFO source=images.go:465 msg="total unused blobs removed: 0"
time=2025-05-02T15:07:07.941-07:00 level=INFO source=routes.go:1298 msg="Listening on [::]:11434 (version 0.6.5)"
time=2025-05-02T15:07:07.943-07:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-05-02T15:07:08.204-07:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-69e8ec11-329c-b37f-418b-2d9ce814ae77 library=cuda variant=v12 compute=6.0 driver=12.4 name="Tesla P100-PCIE-16GB" total="15.9 GiB" available="15.6 GiB"
time=2025-05-02T15:07:08.204-07:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-b85a8f24-13e6-6bfb-96ae-dfd348d658b9 library=cuda variant=v12 compute=6.0 driver=12.4 name="Tesla P100-PCIE-16GB" total="15.9 GiB" available="15.6 GiB"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0time=2025-05-02T15:07:17.696-07:00 level=INFO source=sched.go:732 msg="new model will fit in available VRAM, loading" model=/groups/tylermillhouse/ollama/home/.ollama/models/blobs/sha256-e796792eba26c4d3b04b0ac5adb01a453dd9ec2dfd83b6c59cbf6fe5f30b0f68 library=cuda parallel=4 required="24.3 GiB"
time=2025-05-02T15:07:17.887-07:00 level=INFO source=server.go:105 msg="system memory" total="251.2 GiB" free="246.6 GiB" free_swap="0 B"
time=2025-05-02T15:07:17.890-07:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=63 layers.offload=63 layers.split=32,31 memory.available="[15.6 GiB 15.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="24.3 GiB" memory.required.partial="24.3 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[13.4 GiB 10.8 GiB]" memory.weights.total="15.4 GiB" memory.weights.repeating="14.3 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="1.6 GiB" memory.graph.partial="1.6 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-05-02T15:07:18.020-07:00 level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-05-02T15:07:18.030-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-05-02T15:07:18.030-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-05-02T15:07:18.030-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-05-02T15:07:18.030-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-05-02T15:07:18.030-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-05-02T15:07:18.040-07:00 level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/bin/ollama runner --ollama-engine --model /groups/tylermillhouse/ollama/home/.ollama/models/blobs/sha256-e796792eba26c4d3b04b0ac5adb01a453dd9ec2dfd83b6c59cbf6fe5f30b0f68 --ctx-size 8192 --batch-size 512 --n-gpu-layers 63 --threads 28 --parallel 4 --tensor-split 32,31 --port 45580"
time=2025-05-02T15:07:18.044-07:00 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-05-02T15:07:18.044-07:00 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-05-02T15:07:18.045-07:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-05-02T15:07:18.062-07:00 level=INFO source=runner.go:816 msg="starting ollama engine"
time=2025-05-02T15:07:18.062-07:00 level=INFO source=runner.go:879 msg="Server listening on 127.0.0.1:45580"
time=2025-05-02T15:07:18.183-07:00 level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-05-02T15:07:18.183-07:00 level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-05-02T15:07:18.183-07:00 level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=1247 num_key_values=37
100    56    0     0  100    56      0     55  0:00:01  0:00:01 --:--:--    55time=2025-05-02T15:07:18.298-07:00 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 2 CUDA devices:
  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes
  Device 1: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes
load_backend: loaded CUDA backend from /usr/lib/ollama/cuda_v12/libggml-cuda.so
load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so
time=2025-05-02T15:07:18.971-07:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-05-02T15:07:19.051-07:00 level=INFO source=ggml.go:289 msg="model weights" buffer=CUDA1 size="8.8 GiB"
time=2025-05-02T15:07:19.051-07:00 level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-05-02T15:07:19.051-07:00 level=INFO source=ggml.go:289 msg="model weights" buffer=CUDA0 size="7.4 GiB"
100    56    0     0  100    56      0     27  0:00:02  0:00:02 --:--:--    27100    56    0     0  100    56      0     18  0:00:03  0:00:03 --:--:--    18100    56    0     0  100    56      0     13  0:00:04  0:00:04 --:--:--    13100    56    0     0  100    56      0     11  0:00:05  0:00:05 --:--:--    11100    56    0     0  100    56      0      9  0:00:06  0:00:06 --:--:--     0100    56    0     0  100    56      0      7  0:00:08  0:00:07  0:00:01     0100    56    0     0  100    56      0      6  0:00:09  0:00:08  0:00:01     0100    56    0     0  100    56      0      6  0:00:09  0:00:09 --:--:--     0100    56    0     0  100    56      0      5  0:00:11  0:00:10  0:00:01     0100    56    0     0  100    56      0      5  0:00:11  0:00:11 --:--:--     0100    56    0     0  100    56      0      4  0:00:14  0:00:12  0:00:02     0100    56    0     0  100    56      0      4  0:00:14  0:00:13  0:00:01     0100    56    0     0  100    56      0      3  0:00:18  0:00:14  0:00:04     0100    56    0     0  100    56      0      3  0:00:18  0:00:15  0:00:03     0100    56    0     0  100    56      0      3  0:00:18  0:00:16  0:00:02     0100    56    0     0  100    56      0      3  0:00:18  0:00:17  0:00:01     0100    56    0     0  100    56      0      3  0:00:18  0:00:18 --:--:--     0100    56    0     0  100    56      0      2  0:00:28  0:00:19  0:00:09     0time=2025-05-02T15:07:36.735-07:00 level=INFO source=ggml.go:388 msg="compute graph" backend=CUDA0 buffer_type=CUDA0
time=2025-05-02T15:07:36.735-07:00 level=INFO source=ggml.go:388 msg="compute graph" backend=CUDA1 buffer_type=CUDA1
time=2025-05-02T15:07:36.735-07:00 level=INFO source=ggml.go:388 msg="compute graph" backend=CPU buffer_type=CUDA_Host
time=2025-05-02T15:07:36.739-07:00 level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-05-02T15:07:36.749-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-05-02T15:07:36.749-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-05-02T15:07:36.749-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-05-02T15:07:36.749-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-05-02T15:07:36.749-07:00 level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-05-02T15:07:36.864-07:00 level=INFO source=server.go:619 msg="llama runner started in 18.82 seconds"
100    56    0     0  100    56      0      2  0:00:28  0:00:20  0:00:08     0100   157    0   101  100    56      4      2  0:00:28  0:00:20  0:00:08     9100  1244    0  1188  100    56     54      2  0:00:28  0:00:21  0:00:07   239100  2347    0  2291  100    56    100      2  0:00:28  0:00:22  0:00:06   473100  3961    0  3905  100    56    162      2  0:00:28  0:00:23  0:00:05   797100  3961    0  3905  100    56    162      2  0:00:28  0:00:23  0:00:05  1006
/groups/tylermillhouse/micromamba_envs/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/groups/tylermillhouse/micromamba_envs/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/groups/tylermillhouse/micromamba_envs/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Model: gemma3-27b:   0%|          | 0/79 [00:00<?, ?it/s]Model: gemma3-27b:   1%|▏         | 1/79 [39:03<50:47:07, 2343.94s/it]Model: gemma3-27b:   3%|▎         | 2/79 [1:18:20<50:17:19, 2351.16s/it]Model: gemma3-27b:   4%|▍         | 3/79 [1:59:28<50:45:46, 2404.56s/it]Model: gemma3-27b:   5%|▌         | 4/79 [2:37:20<49:00:14, 2352.19s/it]Model: gemma3-27b:   6%|▋         | 5/79 [3:16:23<48:17:18, 2349.17s/it]Model: gemma3-27b:   8%|▊         | 6/79 [3:59:12<49:08:50, 2423.71s/it]Model: gemma3-27b:   9%|▉         | 7/79 [4:37:42<47:44:00, 2386.68s/it]Model: gemma3-27b:  10%|█         | 8/79 [5:15:40<46:23:19, 2352.11s/it]Model: gemma3-27b:  11%|█▏        | 9/79 [5:53:24<45:11:40, 2324.29s/it]Model: gemma3-27b:  13%|█▎        | 10/79 [6:35:44<45:49:41, 2391.03s/it]Model: gemma3-27b:  14%|█▍        | 11/79 [7:14:28<44:46:39, 2370.59s/it]Model: gemma3-27b:  15%|█▌        | 12/79 [7:55:39<44:41:13, 2401.09s/it]Model: gemma3-27b:  16%|█▋        | 13/79 [8:33:33<43:18:48, 2362.55s/it]Model: gemma3-27b:  18%|█▊        | 14/79 [9:25:24<46:44:11, 2588.48s/it]Model: gemma3-27b:  19%|█▉        | 15/79 [10:07:15<45:36:21, 2565.34s/it]Model: gemma3-27b:  20%|██        | 16/79 [10:56:15<46:51:50, 2677.95s/it]Model: gemma3-27b:  22%|██▏       | 17/79 [11:42:04<46:29:27, 2699.48s/it]Model: gemma3-27b:  23%|██▎       | 18/79 [12:25:10<45:09:46, 2665.35s/it]Model: gemma3-27b:  24%|██▍       | 19/79 [13:04:40<42:56:43, 2576.72s/it]Model: gemma3-27b:  25%|██▌       | 20/79 [13:42:47<40:48:12, 2489.71s/it]Model: gemma3-27b:  27%|██▋       | 21/79 [14:26:14<40:40:48, 2524.97s/it]Model: gemma3-27b:  28%|██▊       | 22/79 [15:05:43<39:14:07, 2478.03s/it]Model: gemma3-27b:  29%|██▉       | 23/79 [15:55:34<40:56:21, 2631.82s/it]Model: gemma3-27b:  30%|███       | 24/79 [16:36:16<39:20:25, 2575.00s/it]Model: gemma3-27b:  32%|███▏      | 25/79 [17:13:00<36:57:20, 2463.70s/it]Model: gemma3-27b:  33%|███▎      | 26/79 [17:58:21<37:24:24, 2540.84s/it]Model: gemma3-27b:  34%|███▍      | 27/79 [18:39:27<36:22:33, 2518.33s/it]Model: gemma3-27b:  35%|███▌      | 28/79 [19:19:49<35:16:12, 2489.65s/it]Model: gemma3-27b:  37%|███▋      | 29/79 [20:02:01<34:45:16, 2502.32s/it]Model: gemma3-27b:  38%|███▊      | 30/79 [20:42:53<33:51:15, 2487.26s/it]Model: gemma3-27b:  39%|███▉      | 31/79 [21:20:38<32:16:20, 2420.42s/it]Model: gemma3-27b:  41%|████      | 32/79 [21:59:55<31:21:13, 2401.57s/it]Model: gemma3-27b:  42%|████▏     | 33/79 [22:37:56<30:13:25, 2365.34s/it]Model: gemma3-27b:  43%|████▎     | 34/79 [23:20:26<30:15:23, 2420.52s/it]Model: gemma3-27b:  44%|████▍     | 35/79 [24:06:42<30:53:21, 2527.30s/it]Model: gemma3-27b:  46%|████▌     | 36/79 [24:47:18<29:51:37, 2499.94s/it]Model: gemma3-27b:  47%|████▋     | 37/79 [25:32:29<29:54:11, 2563.13s/it]Model: gemma3-27b:  48%|████▊     | 38/79 [26:11:37<28:27:30, 2498.79s/it]Model: gemma3-27b:  49%|████▉     | 39/79 [26:52:38<27:38:18, 2487.46s/it]Model: gemma3-27b:  51%|█████     | 40/79 [27:32:47<26:41:27, 2463.77s/it]Model: gemma3-27b:  52%|█████▏    | 41/79 [28:12:39<25:46:47, 2442.30s/it]Model: gemma3-27b:  53%|█████▎    | 42/79 [28:59:50<26:18:03, 2559.02s/it]Model: gemma3-27b:  54%|█████▍    | 43/79 [29:40:47<25:16:58, 2528.30s/it]Model: gemma3-27b:  56%|█████▌    | 44/79 [30:18:29<23:48:13, 2448.39s/it]Model: gemma3-27b:  57%|█████▋    | 45/79 [30:55:34<22:29:24, 2381.30s/it]Model: gemma3-27b:  58%|█████▊    | 46/79 [31:28:43<20:45:03, 2263.76s/it]Model: gemma3-27b:  59%|█████▉    | 47/79 [32:12:18<21:03:29, 2369.04s/it]Model: gemma3-27b:  61%|██████    | 48/79 [33:01:00<21:49:46, 2535.04s/it]Model: gemma3-27b:  62%|██████▏   | 49/79 [33:43:20<21:08:13, 2536.45s/it]Model: gemma3-27b:  63%|██████▎   | 50/79 [34:23:18<20:05:50, 2494.84s/it]Model: gemma3-27b:  65%|██████▍   | 51/79 [35:04:36<19:21:58, 2489.94s/it]Model: gemma3-27b:  66%|██████▌   | 52/79 [35:55:18<19:55:00, 2655.58s/it]Model: gemma3-27b:  67%|██████▋   | 53/79 [36:35:08<18:36:09, 2575.75s/it]Model: gemma3-27b:  68%|██████▊   | 54/79 [37:14:07<17:23:38, 2504.73s/it]Model: gemma3-27b:  70%|██████▉   | 55/79 [38:00:18<17:13:51, 2584.64s/it]Model: gemma3-27b:  71%|███████   | 56/79 [38:39:03<16:00:52, 2506.65s/it]Model: gemma3-27b:  72%|███████▏  | 57/79 [39:21:06<15:20:57, 2511.71s/it]Model: gemma3-27b:  73%|███████▎  | 58/79 [40:08:11<15:12:00, 2605.76s/it]Model: gemma3-27b:  75%|███████▍  | 59/79 [40:49:56<14:18:29, 2575.46s/it]Model: gemma3-27b:  76%|███████▌  | 60/79 [41:38:09<14:05:44, 2670.78s/it]Model: gemma3-27b:  77%|███████▋  | 61/79 [42:19:45<13:05:27, 2618.17s/it]Model: gemma3-27b:  78%|███████▊  | 62/79 [43:04:31<12:27:36, 2638.60s/it]Model: gemma3-27b:  80%|███████▉  | 63/79 [43:46:40<11:34:49, 2605.57s/it]Model: gemma3-27b:  81%|████████  | 64/79 [44:23:48<10:23:06, 2492.43s/it]Model: gemma3-27b:  82%|████████▏ | 65/79 [45:10:47<10:04:25, 2590.39s/it]Model: gemma3-27b:  84%|████████▎ | 66/79 [45:58:01<9:37:03, 2663.35s/it] Model: gemma3-27b:  85%|████████▍ | 67/79 [46:37:51<8:36:18, 2581.55s/it]Model: gemma3-27b:  86%|████████▌ | 68/79 [47:13:22<7:28:31, 2446.47s/it]Model: gemma3-27b:  87%|████████▋ | 69/79 [47:56:03<6:53:27, 2480.71s/it]Model: gemma3-27b:  89%|████████▊ | 70/79 [48:40:48<6:21:18, 2542.06s/it]Model: gemma3-27b:  90%|████████▉ | 71/79 [49:18:56<5:28:47, 2465.90s/it]Model: gemma3-27b:  91%|█████████ | 72/79 [49:57:50<4:43:03, 2426.19s/it]Model: gemma3-27b:  92%|█████████▏| 73/79 [50:36:22<3:59:11, 2391.97s/it]Model: gemma3-27b:  94%|█████████▎| 74/79 [51:17:13<3:20:48, 2409.67s/it]Model: gemma3-27b:  95%|█████████▍| 75/79 [52:02:25<2:46:41, 2500.34s/it]Model: gemma3-27b:  96%|█████████▌| 76/79 [52:41:54<2:03:03, 2461.00s/it]Model: gemma3-27b:  97%|█████████▋| 77/79 [53:18:22<1:19:18, 2379.13s/it]Model: gemma3-27b:  99%|█████████▊| 78/79 [54:02:42<41:03, 2463.31s/it]  Model: gemma3-27b: 100%|██████████| 79/79 [54:43:31<00:00, 2458.96s/it]Model: gemma3-27b: 100%|██████████| 79/79 [54:43:31<00:00, 2493.81s/it]
