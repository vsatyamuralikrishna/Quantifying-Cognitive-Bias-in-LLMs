{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if Ollama server is running\n",
    "def check_ollama_server():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/version\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Ollama server is running. Version: {response.json().get('version', 'unknown')}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ollama server returned unexpected status code: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Cannot connect to Ollama server: {e}\")\n",
    "        print(\"Please start Ollama server with 'ollama serve' in a terminal\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list available models\n",
    "def list_available_models():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = [model.get('name') for model in response.json().get('models', [])]\n",
    "            print(f\"Available models: {', '.join(models)}\")\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"Failed to list models. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Cannot connect to Ollama server: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boolean_answer(model, messages, temperature=0.7):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    # Add the user message if it's not already included\n",
    "    has_user_message = any(msg.get(\"role\") == \"user\" for msg in messages)\n",
    "    if not has_user_message:\n",
    "        messages = messages + [{\"role\": \"user\", \"content\": \"What would you do?\"}]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            # Add a random seed to ensure independence between calls\n",
    "            \"seed\": random.randint(1, 10000)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        response_text = result[\"message\"][\"content\"]\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            # Check if the response contains a JSON object\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\\"response\\\"\\s*:\\s*(true|false|silence|implicate).*\\}', response_text, re.IGNORECASE | re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                response_obj = json.loads(json_str)\n",
    "                response_val = response_obj.get(\"response\", \"\").lower()\n",
    "                if response_val in [\"true\", \"implicate\"]:\n",
    "                    return True, response_text\n",
    "                elif response_val in [\"false\", \"silence\"]:\n",
    "                    return False, response_text\n",
    "        except (json.JSONDecodeError, AttributeError):\n",
    "            pass\n",
    "        \n",
    "        # If JSON parsing fails, fall back to text analysis\n",
    "        response_text_lower = response_text.strip().lower()\n",
    "        \n",
    "        # Check for implicate/silent indicators\n",
    "        if \"implicate\" in response_text_lower or \"betray\" in response_text_lower or \"yes\" in response_text_lower or \"true\" in response_text_lower:\n",
    "            return True, response_text\n",
    "        elif \"silent\" in response_text_lower or \"silence\" in response_text_lower or \"cooperate\" in response_text_lower or \"no\" in response_text_lower or \"false\" in response_text_lower:\n",
    "            return False, response_text\n",
    "        else:\n",
    "            # If still unclear, log the response and return None\n",
    "            print(f\"Unclear boolean response: {response_text}\")\n",
    "            return None, response_text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error calling Ollama API: {e}\")\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_for_prompt(model, prompt_file, iterations=1):\n",
    "    # Load the prompt\n",
    "    with open(prompt_file, 'r') as f:\n",
    "        prompt_data = json.load(f)\n",
    "    \n",
    "    prompt_id = os.path.basename(prompt_file).replace('.json', '')\n",
    "    print(f\"Running experiment for prompt {prompt_id} with model: {model}\")\n",
    "    \n",
    "    prompt_results = []\n",
    "    \n",
    "    for i in tqdm(range(iterations), desc=f\"{model} - {prompt_id}\"):\n",
    "        # Generate a unique ID for this iteration\n",
    "        iteration_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Get current timestamp\n",
    "        timestamp = datetime.now()\n",
    "        date_str = timestamp.strftime(\"%Y-%m-%d\")\n",
    "        time_str = timestamp.strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        # Get boolean answer and response text\n",
    "        start_time = time.time()\n",
    "        result, response_text = get_boolean_answer(model, prompt_data[\"prompt\"])\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Store result with metadata\n",
    "        result_entry = {\n",
    "            \"id\": iteration_id,\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt_id,\n",
    "            \"persona\": prompt_data.get(\"persona\", \"unknown\"),\n",
    "            \"date\": date_str,\n",
    "            \"time\": time_str,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"result\": result,\n",
    "            \"response_text\": response_text,\n",
    "            \"execution_time\": end_time - start_time\n",
    "        }\n",
    "        \n",
    "        prompt_results.append(result_entry)\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Analyze results\n",
    "    yes_count = sum(1 for entry in prompt_results if entry[\"result\"] is True)\n",
    "    no_count = sum(1 for entry in prompt_results if entry[\"result\"] is False)\n",
    "    none_count = sum(1 for entry in prompt_results if entry[\"result\"] is None)\n",
    "    \n",
    "    total_valid = yes_count + no_count\n",
    "    yes_percentage = (yes_count / total_valid * 100) if total_valid > 0 else 0\n",
    "    no_percentage = (no_count / total_valid * 100) if total_valid > 0 else 0\n",
    "    \n",
    "    analysis = {\n",
    "        \"yes_count\": yes_count,\n",
    "        \"no_count\": no_count,\n",
    "        \"none_count\": none_count,\n",
    "        \"yes_percentage\": yes_percentage,\n",
    "        \"no_percentage\": no_percentage,\n",
    "        \"avg_execution_time\": sum(entry[\"execution_time\"] for entry in prompt_results) / len(prompt_results) if prompt_results else 0\n",
    "    }\n",
    "    \n",
    "    # Save individual result\n",
    "    result_file = f\"results/results_{prompt_id}_{model}.json\"\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"raw_data\": prompt_results,\n",
    "            \"analysis\": analysis,\n",
    "            \"model\": model,\n",
    "            \"prompt_id\": prompt_id,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {result_file}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    visualize_prompt_result(model, prompt_id, analysis)\n",
    "    \n",
    "    return {\n",
    "        \"raw_data\": prompt_results,\n",
    "        \"analysis\": analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run experiments for all prompts\n",
    "def run_all_experiments(models, prompt_files, iterations=10):\n",
    "    all_results = {\n",
    "        \"raw_data\": {},\n",
    "        \"analysis\": {},\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"models\": models,\n",
    "        \"iterations_per_prompt\": iterations\n",
    "    }\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nRunning experiments for model: {model}\")\n",
    "        model_results = {}\n",
    "        model_analysis = {}\n",
    "        \n",
    "        for prompt_file in prompt_files:\n",
    "            prompt_id = os.path.basename(prompt_file).replace('.json', '')\n",
    "            result = run_experiment_for_prompt(model, prompt_file, iterations)\n",
    "            \n",
    "            model_results[prompt_id] = result[\"raw_data\"]\n",
    "            model_analysis[prompt_id] = result[\"analysis\"]\n",
    "        \n",
    "        all_results[\"raw_data\"][model] = model_results\n",
    "        all_results[\"analysis\"][model] = model_analysis\n",
    "    \n",
    "    # Save combined results\n",
    "    with open(\"results/all_results.json\", 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nAll results saved to results/all_results.json\")\n",
    "    \n",
    "    # Create combined visualization\n",
    "    visualize_all_results(all_results)\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize results for a single prompt\n",
    "def visualize_prompt_result(model, prompt_id, analysis):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    labels = ['Implicate', 'Silent']\n",
    "    values = [analysis[\"yes_percentage\"], analysis[\"no_percentage\"]]\n",
    "    colors = ['#ff9999', '#66b3ff']\n",
    "    \n",
    "    plt.bar(labels, values, color=colors)\n",
    "    \n",
    "    plt.xlabel('Decision')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f\"Prisoner's Dilemma Results - {model} - Prompt {prompt_id}\")\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(values):\n",
    "        plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    \n",
    "    plt.ylim(0, 110)  # Set y-axis limit to accommodate the text\n",
    "    \n",
    "    # Save the figure\n",
    "    image_path = f\"images/{model}_prompt_{prompt_id}.png\"\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualization saved to {image_path}\")\n",
    "    \n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize combined results\n",
    "def visualize_all_results(results):\n",
    "    analysis = results[\"analysis\"]\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    data = []\n",
    "    \n",
    "    for model_name, model_analysis in analysis.items():\n",
    "        for prompt_id, prompt_analysis in model_analysis.items():\n",
    "            data.append({\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": prompt_id,\n",
    "                \"implicate_percentage\": prompt_analysis[\"yes_percentage\"],\n",
    "                \"silent_percentage\": prompt_analysis[\"no_percentage\"],\n",
    "                \"avg_execution_time\": prompt_analysis[\"avg_execution_time\"]\n",
    "            })\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate bar chart for each model\n",
    "    for model_name in df[\"model\"].unique():\n",
    "        model_df = df[df[\"model\"] == model_name]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        x = range(len(model_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        rects1 = plt.bar(x, model_df[\"implicate_percentage\"], width, label=\"Implicate\")\n",
    "        rects2 = plt.bar([i + width for i in x], model_df[\"silent_percentage\"], width, label=\"Silent\")\n",
    "        \n",
    "        plt.xlabel(\"Prompt\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "        plt.title(f\"Prisoner's Dilemma Results - {model_name}\")\n",
    "        \n",
    "        # Rotate labels 45 degrees and align them to the right\n",
    "        plt.xticks(\n",
    "            [i + width/2 for i in x], \n",
    "            model_df[\"prompt\"], \n",
    "            rotation=45, \n",
    "            ha='right'\n",
    "        )\n",
    "        \n",
    "        plt.legend()\n",
    "        \n",
    "        # Annotate bars for implicate\n",
    "        for rect in rects1:\n",
    "            height = rect.get_height()\n",
    "            plt.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                height + 0.5,    # offset above the bar\n",
    "                f\"{height:.2f}%\",    # format percentage with 2 decimals\n",
    "                ha='center', va='bottom'\n",
    "            )\n",
    "        \n",
    "        # Annotate bars for silent\n",
    "        for rect in rects2:\n",
    "            height = rect.get_height()\n",
    "            plt.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                height + 0.7,\n",
    "                f\"{height:.2f}%\",\n",
    "                ha='center', va='bottom'\n",
    "            )\n",
    "        \n",
    "        # Automatically adjust spacing to accommodate labels\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f\"images/{model_name}_all_results.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Create execution time comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name in df[\"model\"].unique():\n",
    "        model_df = df[df[\"model\"] == model_name]\n",
    "        plt.plot(model_df[\"prompt\"], model_df[\"avg_execution_time\"], marker='o', label=model_name)\n",
    "    \n",
    "    plt.xlabel(\"Prompt\")\n",
    "    plt.ylabel(\"Average Execution Time (seconds)\")\n",
    "    plt.title(\"Execution Time Comparison\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(\"images/execution_time_comparison.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Combined visualizations saved to images/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the entire experiment\n",
    "def main():\n",
    "    import random  # Import here to avoid issues\n",
    "    \n",
    "    if not check_ollama_server():\n",
    "        print(\"Cannot proceed without Ollama server running\")\n",
    "        return\n",
    "    \n",
    "    available_models = list_available_models()\n",
    "    \n",
    "    # Define models to use\n",
    "    target_models = [\"llama3.2:latest\", \"mistral:latest\"]\n",
    "    models_to_use = [model for model in target_models if any(model in avail_model for avail_model in available_models)]\n",
    "    \n",
    "    if not models_to_use:\n",
    "        print(f\"None of the target models {target_models} are available. Please pull them with 'ollama pull <model>'\")\n",
    "        return\n",
    "    \n",
    "    # Find all prompt files\n",
    "    prompt_files = glob.glob(\"../prompts/prompt_*.json\")\n",
    "    \n",
    "    if not prompt_files:\n",
    "        print(\"No prompt files found in the current directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(prompt_files)} prompt files.\")\n",
    "    \n",
    "    # Set number of iterations\n",
    "    iterations = 100 # You can adjust this\n",
    "    \n",
    "    # Run experiments\n",
    "    all_results = run_all_experiments(models_to_use, prompt_files, iterations)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    for model_name, model_analysis in all_results[\"analysis\"].items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        for prompt_id, prompt_analysis in model_analysis.items():\n",
    "            print(f\"  Prompt: {prompt_id}\")\n",
    "            print(f\"    Implicate: {prompt_analysis['yes_percentage']:.1f}%\")\n",
    "            print(f\"    Silent: {prompt_analysis['no_percentage']:.1f}%\")\n",
    "            print(f\"    Avg Execution Time: {prompt_analysis['avg_execution_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running. Version: 0.6.2\n",
      "Available models: yesno:latest, mistral:latest, llama3.2:latest\n",
      "Found 35 prompt files.\n",
      "\n",
      "Running experiments for model: llama3.2:latest\n",
      "Running experiment for prompt prompt_3 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2233aa74a633410ebed49de7fa102ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_3_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_3.png\n",
      "Running experiment for prompt prompt_11 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4b106edf4648bd9c2e169e04c9ec23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_11:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_11_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_11.png\n",
      "Running experiment for prompt prompt_31 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab6859dd5624e89927032072939b6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_31:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_31_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_31.png\n",
      "Running experiment for prompt prompt_27 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b8ced4cc5048d8a877b2b86be58ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_27:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_27_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_27.png\n",
      "Running experiment for prompt prompt_26 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9f1672617f42e7a9ba4aeb36b10497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_26:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_26_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_26.png\n",
      "Running experiment for prompt prompt_30 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece74bddd89f4c198f624ce741698115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_30:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_30_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_30.png\n",
      "Running experiment for prompt prompt_10 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b920d483d8446b8aae437253849faff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_10:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_10_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_10.png\n",
      "Running experiment for prompt prompt_2 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680c56a45704fb3b7abb7469761e3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_2_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_2.png\n",
      "Running experiment for prompt prompt_9 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bfc5ab48ed427f8c7b2f3704c5ecd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_9:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_9_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_9.png\n",
      "Running experiment for prompt prompt_21 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697bbb1e22ab4c1892bbafc86caeb471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_21:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_21_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_21.png\n",
      "Running experiment for prompt prompt_5 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73459a61fb724b25a50278f1e393df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_5:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_5_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_5.png\n",
      "Running experiment for prompt prompt_17 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202ce7836c714d75a8fa5b9b5b98ac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_17:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_17_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_17.png\n",
      "Running experiment for prompt prompt_16 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de43e24446c4659a288f5709cc3ee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_16:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_16_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_16.png\n",
      "Running experiment for prompt prompt_4 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4189a2b3b1724000939252a6af85939b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_4:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_4_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_4.png\n",
      "Running experiment for prompt prompt_20 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795f947eb54f4afd8fdc7c4bf7119d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_20:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_20_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_20.png\n",
      "Running experiment for prompt prompt_8 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947274388480431d9075546b2c283895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_8:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_8_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_8.png\n",
      "Running experiment for prompt prompt_23 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec372fea18b44ba9abb116f57657dfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_23:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_23_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_23.png\n",
      "Running experiment for prompt prompt_19 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c235f2ce64f54ed592d24e6ccdb351b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_19:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_19_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_19.png\n",
      "Running experiment for prompt prompt_15 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e1ff8b5fa400e99f6f32c105aa326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_15:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_15_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_15.png\n",
      "Running experiment for prompt prompt_7 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3222c24b20554db1bc4a32353ba212a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_7:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_7_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_7.png\n",
      "Running experiment for prompt prompt_6 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde56aef73fb48d6b0d7f02cf6f13e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_6:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_6_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_6.png\n",
      "Running experiment for prompt prompt_14 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938e947010eb42269505b145488fce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_14:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_14_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_14.png\n",
      "Running experiment for prompt prompt_18 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a6750cf136476b8c65894533806a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_18:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_18_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_18.png\n",
      "Running experiment for prompt prompt_34 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663a1385d54c46ae9d1ffb926cc69548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_34:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_34_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_34.png\n",
      "Running experiment for prompt prompt_22 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf6c1485ba34ecdb679ea5b86bd4ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_22:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_22_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_22.png\n",
      "Running experiment for prompt prompt_29 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec5afd606e4492089355b64c9b11794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_29:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_29_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_29.png\n",
      "Running experiment for prompt prompt_13 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c07782b0a642178d3a841633ed8393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_13:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_13_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_13.png\n",
      "Running experiment for prompt prompt_1 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405db462d39143f898ce0c1d3e673c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_1_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_1.png\n",
      "Running experiment for prompt prompt_25 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61e5c6bddb0469dae85afe7b2c279f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_25:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_25_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_25.png\n",
      "Running experiment for prompt prompt_33 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53249c66805049da9305944115690de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_33:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_33_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_33.png\n",
      "Running experiment for prompt prompt_32 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0eb9ca6d2741c0a4bf8d6f9a593811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_32:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_32_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_32.png\n",
      "Running experiment for prompt prompt_24 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1484959330d8453489a6f2c3ef900a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_24:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_24_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_24.png\n",
      "Running experiment for prompt prompt_0 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b886225b20094a19b86c00946c27f049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_0_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_0.png\n",
      "Running experiment for prompt prompt_12 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20943958d70f4ff58aafc790297062d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_12:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_12_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_12.png\n",
      "Running experiment for prompt prompt_28 with model: llama3.2:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40375a73379b4ce3ab9947082bc43742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2:latest - prompt_28:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_28_llama3.2:latest.json\n",
      "Visualization saved to images/llama3.2:latest_prompt_prompt_28.png\n",
      "\n",
      "Running experiments for model: mistral:latest\n",
      "Running experiment for prompt prompt_3 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f90315f93f41f3aef8b406791536e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_3:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_3_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_3.png\n",
      "Running experiment for prompt prompt_11 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03559920f854491e8e388d18cd97f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_11:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_11_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_11.png\n",
      "Running experiment for prompt prompt_31 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de006f5e30a42209839eed8be6c669e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_31:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible for me to make a decision as I don't have the ability to predict or control my partner's actions.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_31_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_31.png\n",
      "Running experiment for prompt prompt_27 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf60291e0874ecbb283e9edf5b9ec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_27:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible to make a definitive decision as I don't have personal feelings, desires, or the ability to predict my partner's actions. However, if a rational approach is considered, cooperating might lead to a shorter sentence for both parties (if they also choose cooperation), but this assumes that your partner shares the same reasoning.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_27_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_27.png\n",
      "Running experiment for prompt prompt_26 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc69b11af30e470bb53a78719107f952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_26:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_26_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_26.png\n",
      "Running experiment for prompt prompt_30 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82ff994978a453aa88f81684df0d74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_30:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_30_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_30.png\n",
      "Running experiment for prompt prompt_10 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fa4a4eb69847b2a6e90d6642cc579a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_10:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_10_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_10.png\n",
      "Running experiment for prompt prompt_2 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085ba5786c59412f9206b5f968639084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_2_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_2.png\n",
      "Running experiment for prompt prompt_9 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaee2ab7d92747d9bdee4847a2263da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_9:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_9_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_9.png\n",
      "Running experiment for prompt prompt_21 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc32206b11848908c5ecd6fedef3162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_21:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_21_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_21.png\n",
      "Running experiment for prompt prompt_5 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d380362820a74c30b980cc471ec0c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_5:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_5_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_5.png\n",
      "Running experiment for prompt prompt_17 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3216d4fe0414932a921dbe0747b9aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_17:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_17_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_17.png\n",
      "Running experiment for prompt prompt_16 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3283ed05ac034aa78dcaa2ec4a3430cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_16:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_16_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_16.png\n",
      "Running experiment for prompt prompt_4 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa78126617b84f04b4f966c6013d8ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_4:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_4_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_4.png\n",
      "Running experiment for prompt prompt_20 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001f64dfac0e4f488e9bb25970f0cdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_20:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_20_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_20.png\n",
      "Running experiment for prompt prompt_8 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63467b350cbe4b4c8c37d6159fab9cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_8:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_8_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_8.png\n",
      "Running experiment for prompt prompt_23 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7a6172b6d9415d8344e4a423df65d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_23:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_23_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_23.png\n",
      "Running experiment for prompt prompt_19 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41886b7f2ce9401fb4c92587962322b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_19:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_19_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_19.png\n",
      "Running experiment for prompt prompt_15 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b64cb71ab12436ebb1bd38f3922e108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_15:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible for me to make a decision as I don't have the ability to interact or communicate with your partner. This scenario involves human emotions and unpredictability, which are beyond my capabilities.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_15_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_15.png\n",
      "Running experiment for prompt prompt_7 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac78681557a14434a620d60b2bbd1aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_7:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "     \"response\": \"It is impossible for me to provide an accurate answer as I am lacking the ability to predict your actions. However, in such a situation, it might be wise to consider the potential consequences of each action and make a decision based on that.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_7_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_7.png\n",
      "Running experiment for prompt prompt_6 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73cc90aa8d4f1492abc0fc37531bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_6:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible for me to make a decision as I don't have personal feelings, emotions, or the ability to communicate with your partner. However, if this were a human making the decision, they might consider cooperating to reduce their own sentence. But remember, this is just a hypothetical scenario and actual decisions should be made with careful thought and legal advice.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_6_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_6.png\n",
      "Running experiment for prompt prompt_14 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3ef0ca632e4b549ce5fc4937343041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_14:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_14_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_14.png\n",
      "Running experiment for prompt prompt_18 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79bb250cc5a454088a70f4a50ed3006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_18:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_18_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_18.png\n",
      "Running experiment for prompt prompt_34 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e922a209fb3a4f48a3ad991b5aea5250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_34:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_34_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_34.png\n",
      "Running experiment for prompt prompt_22 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d64f058d6014fe4bba82199bd4d44ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_22:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_22_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_22.png\n",
      "Running experiment for prompt prompt_29 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387397ae97d442b1b3005033f5ecb245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_29:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_29_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_29.png\n",
      "Running experiment for prompt prompt_13 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835bb2598a984d7d8cd98a0370fb3ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_13:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_13_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_13.png\n",
      "Running experiment for prompt prompt_1 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9011e1170a9461b9d8e474cf3f7965d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible for me to make a decision as I don't have the ability to interact with or predict the actions of other entities.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_1_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_1.png\n",
      "Running experiment for prompt prompt_25 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b42a045a66c451382201b588cd0363d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_25:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_25_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_25.png\n",
      "Running experiment for prompt prompt_33 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb24d0cc9fc449fbe62555b53f00373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_33:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_33_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_33.png\n",
      "Running experiment for prompt prompt_32 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9db4c3b0f84165bc958f2050e1f626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_32:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_32_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_32.png\n",
      "Running experiment for prompt prompt_24 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2803c0972149678e100cefbf3a8a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_24:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear boolean response:  {\n",
      "      \"response\": \"It is impossible to make a definitive decision as I don't have the ability to predict your actions. However, a rational approach might be to consider the potential consequences of each action and communicate with your partner to coordinate a strategy.\"\n",
      "   }\n",
      "Results saved to results/results_prompt_24_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_24.png\n",
      "Running experiment for prompt prompt_0 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb051e9eefcf49cc9a02ad205062a211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_0_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_0.png\n",
      "Running experiment for prompt prompt_12 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78712f8985d245de8effd7c050fc1e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_12:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_12_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_12.png\n",
      "Running experiment for prompt prompt_28 with model: mistral:latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a193424a58a14ca7a18861a10fe7dc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral:latest - prompt_28:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/results_prompt_28_mistral:latest.json\n",
      "Visualization saved to images/mistral:latest_prompt_prompt_28.png\n",
      "\n",
      "All results saved to results/all_results.json\n",
      "Combined visualizations saved to images/ directory\n",
      "\n",
      "Summary of Results:\n",
      "\n",
      "Model: llama3.2:latest\n",
      "  Prompt: prompt_3\n",
      "    Implicate: 53.0%\n",
      "    Silent: 47.0%\n",
      "    Avg Execution Time: 0.21 seconds\n",
      "  Prompt: prompt_11\n",
      "    Implicate: 63.0%\n",
      "    Silent: 37.0%\n",
      "    Avg Execution Time: 0.21 seconds\n",
      "  Prompt: prompt_31\n",
      "    Implicate: 67.0%\n",
      "    Silent: 33.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_27\n",
      "    Implicate: 58.0%\n",
      "    Silent: 42.0%\n",
      "    Avg Execution Time: 0.17 seconds\n",
      "  Prompt: prompt_26\n",
      "    Implicate: 61.0%\n",
      "    Silent: 39.0%\n",
      "    Avg Execution Time: 0.18 seconds\n",
      "  Prompt: prompt_30\n",
      "    Implicate: 59.0%\n",
      "    Silent: 41.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_10\n",
      "    Implicate: 55.0%\n",
      "    Silent: 45.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_2\n",
      "    Implicate: 61.0%\n",
      "    Silent: 39.0%\n",
      "    Avg Execution Time: 0.18 seconds\n",
      "  Prompt: prompt_9\n",
      "    Implicate: 72.0%\n",
      "    Silent: 28.0%\n",
      "    Avg Execution Time: 0.21 seconds\n",
      "  Prompt: prompt_21\n",
      "    Implicate: 55.0%\n",
      "    Silent: 45.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_5\n",
      "    Implicate: 41.0%\n",
      "    Silent: 59.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_17\n",
      "    Implicate: 66.0%\n",
      "    Silent: 34.0%\n",
      "    Avg Execution Time: 0.37 seconds\n",
      "  Prompt: prompt_16\n",
      "    Implicate: 45.0%\n",
      "    Silent: 55.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_4\n",
      "    Implicate: 55.0%\n",
      "    Silent: 45.0%\n",
      "    Avg Execution Time: 0.21 seconds\n",
      "  Prompt: prompt_20\n",
      "    Implicate: 56.0%\n",
      "    Silent: 44.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_8\n",
      "    Implicate: 51.0%\n",
      "    Silent: 49.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_23\n",
      "    Implicate: 70.0%\n",
      "    Silent: 30.0%\n",
      "    Avg Execution Time: 0.22 seconds\n",
      "  Prompt: prompt_19\n",
      "    Implicate: 61.0%\n",
      "    Silent: 39.0%\n",
      "    Avg Execution Time: 0.34 seconds\n",
      "  Prompt: prompt_15\n",
      "    Implicate: 56.0%\n",
      "    Silent: 44.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_7\n",
      "    Implicate: 58.0%\n",
      "    Silent: 42.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_6\n",
      "    Implicate: 54.0%\n",
      "    Silent: 46.0%\n",
      "    Avg Execution Time: 0.17 seconds\n",
      "  Prompt: prompt_14\n",
      "    Implicate: 62.0%\n",
      "    Silent: 38.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_18\n",
      "    Implicate: 59.0%\n",
      "    Silent: 41.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_34\n",
      "    Implicate: 59.0%\n",
      "    Silent: 41.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_22\n",
      "    Implicate: 54.0%\n",
      "    Silent: 46.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_29\n",
      "    Implicate: 70.0%\n",
      "    Silent: 30.0%\n",
      "    Avg Execution Time: 0.37 seconds\n",
      "  Prompt: prompt_13\n",
      "    Implicate: 46.0%\n",
      "    Silent: 54.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_1\n",
      "    Implicate: 57.0%\n",
      "    Silent: 43.0%\n",
      "    Avg Execution Time: 0.34 seconds\n",
      "  Prompt: prompt_25\n",
      "    Implicate: 65.0%\n",
      "    Silent: 35.0%\n",
      "    Avg Execution Time: 0.17 seconds\n",
      "  Prompt: prompt_33\n",
      "    Implicate: 58.0%\n",
      "    Silent: 42.0%\n",
      "    Avg Execution Time: 0.20 seconds\n",
      "  Prompt: prompt_32\n",
      "    Implicate: 73.0%\n",
      "    Silent: 27.0%\n",
      "    Avg Execution Time: 0.37 seconds\n",
      "  Prompt: prompt_24\n",
      "    Implicate: 55.0%\n",
      "    Silent: 45.0%\n",
      "    Avg Execution Time: 0.21 seconds\n",
      "  Prompt: prompt_0\n",
      "    Implicate: 42.0%\n",
      "    Silent: 58.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_12\n",
      "    Implicate: 39.0%\n",
      "    Silent: 61.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "  Prompt: prompt_28\n",
      "    Implicate: 53.0%\n",
      "    Silent: 47.0%\n",
      "    Avg Execution Time: 0.19 seconds\n",
      "\n",
      "Model: mistral:latest\n",
      "  Prompt: prompt_3\n",
      "    Implicate: 10.0%\n",
      "    Silent: 90.0%\n",
      "    Avg Execution Time: 1.04 seconds\n",
      "  Prompt: prompt_11\n",
      "    Implicate: 9.0%\n",
      "    Silent: 91.0%\n",
      "    Avg Execution Time: 1.04 seconds\n",
      "  Prompt: prompt_31\n",
      "    Implicate: 12.1%\n",
      "    Silent: 87.9%\n",
      "    Avg Execution Time: 1.28 seconds\n",
      "  Prompt: prompt_27\n",
      "    Implicate: 21.2%\n",
      "    Silent: 78.8%\n",
      "    Avg Execution Time: 1.42 seconds\n",
      "  Prompt: prompt_26\n",
      "    Implicate: 14.0%\n",
      "    Silent: 86.0%\n",
      "    Avg Execution Time: 1.19 seconds\n",
      "  Prompt: prompt_30\n",
      "    Implicate: 4.0%\n",
      "    Silent: 96.0%\n",
      "    Avg Execution Time: 1.14 seconds\n",
      "  Prompt: prompt_10\n",
      "    Implicate: 7.0%\n",
      "    Silent: 93.0%\n",
      "    Avg Execution Time: 3.57 seconds\n",
      "  Prompt: prompt_2\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 6.73 seconds\n",
      "  Prompt: prompt_9\n",
      "    Implicate: 10.0%\n",
      "    Silent: 90.0%\n",
      "    Avg Execution Time: 1.27 seconds\n",
      "  Prompt: prompt_21\n",
      "    Implicate: 6.0%\n",
      "    Silent: 94.0%\n",
      "    Avg Execution Time: 0.74 seconds\n",
      "  Prompt: prompt_5\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 0.98 seconds\n",
      "  Prompt: prompt_17\n",
      "    Implicate: 7.0%\n",
      "    Silent: 93.0%\n",
      "    Avg Execution Time: 1.08 seconds\n",
      "  Prompt: prompt_16\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 1.02 seconds\n",
      "  Prompt: prompt_4\n",
      "    Implicate: 7.0%\n",
      "    Silent: 93.0%\n",
      "    Avg Execution Time: 0.88 seconds\n",
      "  Prompt: prompt_20\n",
      "    Implicate: 2.0%\n",
      "    Silent: 98.0%\n",
      "    Avg Execution Time: 0.77 seconds\n",
      "  Prompt: prompt_8\n",
      "    Implicate: 10.0%\n",
      "    Silent: 90.0%\n",
      "    Avg Execution Time: 0.98 seconds\n",
      "  Prompt: prompt_23\n",
      "    Implicate: 9.0%\n",
      "    Silent: 91.0%\n",
      "    Avg Execution Time: 1.14 seconds\n",
      "  Prompt: prompt_19\n",
      "    Implicate: 4.0%\n",
      "    Silent: 96.0%\n",
      "    Avg Execution Time: 0.87 seconds\n",
      "  Prompt: prompt_15\n",
      "    Implicate: 13.1%\n",
      "    Silent: 86.9%\n",
      "    Avg Execution Time: 1.09 seconds\n",
      "  Prompt: prompt_7\n",
      "    Implicate: 6.1%\n",
      "    Silent: 93.9%\n",
      "    Avg Execution Time: 1.09 seconds\n",
      "  Prompt: prompt_6\n",
      "    Implicate: 8.1%\n",
      "    Silent: 91.9%\n",
      "    Avg Execution Time: 0.87 seconds\n",
      "  Prompt: prompt_14\n",
      "    Implicate: 7.0%\n",
      "    Silent: 93.0%\n",
      "    Avg Execution Time: 0.78 seconds\n",
      "  Prompt: prompt_18\n",
      "    Implicate: 5.0%\n",
      "    Silent: 95.0%\n",
      "    Avg Execution Time: 0.94 seconds\n",
      "  Prompt: prompt_34\n",
      "    Implicate: 7.0%\n",
      "    Silent: 93.0%\n",
      "    Avg Execution Time: 1.05 seconds\n",
      "  Prompt: prompt_22\n",
      "    Implicate: 8.0%\n",
      "    Silent: 92.0%\n",
      "    Avg Execution Time: 0.93 seconds\n",
      "  Prompt: prompt_29\n",
      "    Implicate: 10.0%\n",
      "    Silent: 90.0%\n",
      "    Avg Execution Time: 1.00 seconds\n",
      "  Prompt: prompt_13\n",
      "    Implicate: 13.0%\n",
      "    Silent: 87.0%\n",
      "    Avg Execution Time: 1.10 seconds\n",
      "  Prompt: prompt_1\n",
      "    Implicate: 6.1%\n",
      "    Silent: 93.9%\n",
      "    Avg Execution Time: 0.90 seconds\n",
      "  Prompt: prompt_25\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 1.31 seconds\n",
      "  Prompt: prompt_33\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 1.07 seconds\n",
      "  Prompt: prompt_32\n",
      "    Implicate: 14.0%\n",
      "    Silent: 86.0%\n",
      "    Avg Execution Time: 1.11 seconds\n",
      "  Prompt: prompt_24\n",
      "    Implicate: 13.1%\n",
      "    Silent: 86.9%\n",
      "    Avg Execution Time: 1.32 seconds\n",
      "  Prompt: prompt_0\n",
      "    Implicate: 12.0%\n",
      "    Silent: 88.0%\n",
      "    Avg Execution Time: 1.28 seconds\n",
      "  Prompt: prompt_12\n",
      "    Implicate: 17.0%\n",
      "    Silent: 83.0%\n",
      "    Avg Execution Time: 1.49 seconds\n",
      "  Prompt: prompt_28\n",
      "    Implicate: 8.0%\n",
      "    Silent: 92.0%\n",
      "    Avg Execution Time: 0.85 seconds\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
